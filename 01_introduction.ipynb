{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhpEWeGNVo1v"
      },
      "source": [
        "# Chapter 01 - Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fundamentals Concepts to understand the novelty around transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JN2bJ5UV1p0"
      },
      "source": [
        "To understand what transformers are, we first need to understand the novelties behind it.\n",
        "\n",
        "1. The encode-decoder framework\n",
        "\n",
        "The encoder-decoder framework is based on the RNN architecture developed for multilingual translation. The encoder block receives a text sentence (in the first language, e.g., english) and encode it to numerical format into what is called \"hidden state\". The hidden state is then passed to the decoder block, the decodes it into the target language (e.g., german).\n",
        "\n",
        "2. Attention Mechanisms\n",
        "\n",
        "Due to the inhererent sequential natural of the RNNs, for long sequences, after feeding the whole sequence to the RNN, the resulting hidden state may lose the initial context of the sentence. To overcome this issue, \"attention\" mechanisms were invented. The attention mechanisms idea is that instead of passing only the final hidden state to the decoder, we should pass all of them, from the first to the last. This enable the model to learn non-trivial relationship between words and to overcome the memory issue of forgetting initial information. \n",
        "\n",
        "Lastly, one last challenge remained: the inherent sequential nature of RNN. To mitigate that problem and training models faster, the transformer architecture was introduced with the idea of \"self-attention\". Self-attention basically inputs words to Feed-Forward Neural Networks (FF NN), to generate and to decode the hidden states of the transformer network.\n",
        "\n",
        "3. Transfer Learning\n",
        "\n",
        "Transfer Learning in NLP is powered by the ULMFiT framework. Due to the lack of labeled data for NLP tasks, researchers from OpenAI proposed the ULMFiT framework that is basically three steps:\n",
        "\n",
        "- Pretraining: Get a large corpus of data such as wikipedia, and train a model to predict the next word based on the previous one. This task is referred to as \"language modeling\".\n",
        "- Domain adaptation: Still uses language modeling but over texts from the domain you want to work with (fine-tuning).\n",
        "- Fine-Tuning: now that the model has been fine-tuned, we can put a classification head over it (or whatever we need), and perform the target task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import warnings\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hugging Face Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "classifier = pipeline(\"text-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.901546</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label     score\n",
              "0  NEGATIVE  0.901546"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure\n",
        "from your online store in Germany. Unfortunately, when I opened the package,\n",
        "I discovered to my horror that I had been sent an action figure of Megatron\n",
        "instead! As a lifelong enemy of the Decepticons, I hope you can understand my\n",
        "dilemma. To resolve the issue, I demand an exchange of Megatron for the\n",
        "Optimus Prime figure I ordered. Enclosed are copies of my records concerning\n",
        "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n",
        "\n",
        "outputs = classifier(text)\n",
        "pd.DataFrame(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entity_group</th>\n",
              "      <th>score</th>\n",
              "      <th>word</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ORG</td>\n",
              "      <td>0.879010</td>\n",
              "      <td>Amazon</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.990859</td>\n",
              "      <td>Optimus Prime</td>\n",
              "      <td>36</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LOC</td>\n",
              "      <td>0.999755</td>\n",
              "      <td>Germany</td>\n",
              "      <td>90</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.556569</td>\n",
              "      <td>Mega</td>\n",
              "      <td>208</td>\n",
              "      <td>212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PER</td>\n",
              "      <td>0.590256</td>\n",
              "      <td>##tron</td>\n",
              "      <td>212</td>\n",
              "      <td>216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ORG</td>\n",
              "      <td>0.669692</td>\n",
              "      <td>Decept</td>\n",
              "      <td>253</td>\n",
              "      <td>259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.498350</td>\n",
              "      <td>##icons</td>\n",
              "      <td>259</td>\n",
              "      <td>264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.775362</td>\n",
              "      <td>Megatron</td>\n",
              "      <td>350</td>\n",
              "      <td>358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MISC</td>\n",
              "      <td>0.987854</td>\n",
              "      <td>Optimus Prime</td>\n",
              "      <td>367</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>PER</td>\n",
              "      <td>0.812096</td>\n",
              "      <td>Bumblebee</td>\n",
              "      <td>502</td>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  entity_group     score           word  start  end\n",
              "0          ORG  0.879010         Amazon      5   11\n",
              "1         MISC  0.990859  Optimus Prime     36   49\n",
              "2          LOC  0.999755        Germany     90   97\n",
              "3         MISC  0.556569           Mega    208  212\n",
              "4          PER  0.590256         ##tron    212  216\n",
              "5          ORG  0.669692         Decept    253  259\n",
              "6         MISC  0.498350        ##icons    259  264\n",
              "7         MISC  0.775362       Megatron    350  358\n",
              "8         MISC  0.987854  Optimus Prime    367  380\n",
              "9          PER  0.812096      Bumblebee    502  511"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = ner_tagger(text)\n",
        "pd.DataFrame(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
            "\n",
            "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.631292</td>\n",
              "      <td>335</td>\n",
              "      <td>358</td>\n",
              "      <td>an exchange of Megatron</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      score  start  end                   answer\n",
              "0  0.631292    335  358  an exchange of Megatron"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reader = pipeline(\"question-answering\")\n",
        "question = \"What does the customer want?\"\n",
        "outputs = reader(question=question, context=text)\n",
        "\n",
        "pd.DataFrame([outputs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1716894334.255489   30501 service.cc:145] XLA service 0x7fb65f1be850 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1716894334.255541   30501 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
            "2024-05-28 08:05:34.255860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-28 08:05:34.261254: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1716894334.286034   30501 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        }
      ],
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'last week, I ordered an Optimus Prime action figure from your online store in germany. when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[0][\"summary_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-de.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7da4e3beacfa4716ab110f044553d984",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4660c66f15aa4fefa4c74bfe4b1c66f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1c191fb65d84af8965325a9248d0770",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "\n",
        "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket öffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie können mein Dilemma verstehen. Um das Problem zu lösen, Ich fordere einen Austausch von Megatron für die Optimus Prime Figur bestellte ich. Anbei sind Kopien meiner Aufzeichnungen über diesen Kauf. Ich erwarte, von Ihnen bald zu hören. Aufrichtig, Bumblebee.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[0][\"translation_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
        "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
        "outputs = generator(prompt, max_length=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dear Amazon, last week I ordered an Optimus Prime action figure\n",
            "from your online store in Germany. Unfortunately, when I opened the package,\n",
            "I discovered to my horror that I had been sent an action figure of Megatron\n",
            "instead! As a lifelong enemy of the Decepticons, I hope you can understand my\n",
            "dilemma. To resolve the issue, I demand an exchange of Megatron for the\n",
            "Optimus Prime figure I ordered. Enclosed are copies of my records concerning\n",
            "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
            "\n",
            "Customer service response:\n",
            "Dear Bumblebee, I am sorry to hear that your order was mixed up. It was from your online store, not\n",
            "\n",
            "from another supplier. I do not know where I got your code, or if it is an e-mail address or fax machine (it was\n",
            "\n",
            "or was received by your company). Also, I have been sent with what looks like\n"
          ]
        }
      ],
      "source": [
        "print(outputs[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
